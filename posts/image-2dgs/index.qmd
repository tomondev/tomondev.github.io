---
title: "2D Gaussian Splatting and Image Rendering"
date: 2025-01-01
author: Tom
---

[3D Gaussian Splatting](https://huggingface.co/blog/gaussian-splatting) (3DGS) is a rendering technique for photorealistic view synthesis. In this post, we create a toy example of 2D Gaussian splatting (2DGS). We train a 2DGS model to render a small image. This 2D version removes much of the complexity found in 3DGS, making the core ideas easier to understand.

Here are some key differences between 2DGS and 3DGS:

- A single image is sufficient to train a 2DGS model, unlike 3DGS, which often requires multiple views.
- 2DGS does not require handling camera poses or parameters.
- In 2DGS, a single color is assigned to each Gaussian, whereas 3DGS uses spherical harmonics to model view-dependent appearance.
- In 2DGS, pixel colors are determined by a simple weighted average of Gaussian colors, eliminating the need for depth ordering or opacity attributes required in 3DGS's alpha-blended rendering.

## Problem Formulation
A 2D Gaussian has a mean vector $\mu\in\mathbb{R}^2$ and a covariance matrix $\Sigma\in\mathbb{R}^{2 \times 2}$. The density is
$$
G(x)=\exp\left(-\dfrac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right),
$$
where $x$ is the pixel location. This is not a probability density, so it does not need to be normalized.

A common parameterization uses $\Sigma = R S S^T R^T$ to ensure $\Sigma$ is always positive semi-definite during training. $R\in\mathbb{R}^{2\times2}$ is a rotation matrix parameterized by an angle $\theta\in[0,\pi]$, and $S\in\mathbb{R}^{2\times2}$ is a scaling matrix determined by a scaling vector $s\in\mathbb{R}^2_+$.

Next, we assign a color to each Gaussian. Unlike 3DGS, which uses spherical harmonics, we only need a vector $c\in\mathbb{R}^3$ to represent RGB values.

Thus, a 2D Gaussian primitive is characterized by $(\mu, \theta, s, c)$. Each pixel value $\mathbf{RGB}(x)$ is a weighted average of all Gaussians, with weights given by $G_i(x)$:
$$
\mathbf{RGB}(x)=\dfrac{1}{\sum_i G_i(x)}\sum_i G_i(x)c_i.
$$

## Implementation

First, we randomly initialize all Gaussian parameters. We use 5000 Gaussians. Using more Gaussians improves quality but increases memory usage.

```python
_GS_NUM = 5000
key = jax.random.PRNGKey(0)
key_mu, key_theta, key_scaling, key_color = jax.random.split(key, 4)
mu_array = jax.random.uniform(key_mu, (_GS_NUM, 2), minval=0.0, maxval=1.0)
theta_array = jax.random.uniform(key_theta, (_GS_NUM, 1), minval=0.0, maxval=jnp.pi)
scaling_array = jax.random.uniform(
    key_scaling, (_GS_NUM, 2), minval=0.0, maxval=0.1
)
color_array = jax.random.uniform(key_color, (_GS_NUM, 3), minval=0.0, maxval=255.0)
```

Next, we implement the key part of the training code: a function to compute the Gaussian density. We implement the function for a single Gaussian at a single pixel location, then use three layers of `jax.vmap` to vectorize it so one call computes the density for all Gaussians at all locations.

```python
@partial(jax.vmap, in_axes=(None, None, None, 0))  # map over height
@partial(jax.vmap, in_axes=(None, None, None, 0))  # map over width
@partial(jax.vmap, in_axes=(0, 0, 0, None))        # map over Gaussians
def compute_gaussians(mu, theta, scaling, coord):
    """Compute the value of a single Gaussian at the given coordinate."""
    # rotation matrix
    theta = jnp.clip(theta, 0.0, jnp.pi)
    c = jnp.cos(theta[0])
    s = jnp.sin(theta[0])
    R = jnp.array([[c, -s], [s, c]])
    # scaling matrix
    scaling = jnp.clip(scaling, min=1e-6)
    S = jnp.diag(scaling)
    # covariance matrix
    Sigma = R @ S @ S @ R.T + 1e-6 * jnp.eye(2)
    diff = coord - mu
    exponent = -0.5 * diff @ jnp.linalg.inv(Sigma) @ diff.T
    return jnp.exp(exponent)
```

With `compute_gaussians` implemented, we finish the rendering function. It computes a weighted average of all Gaussian colors for each pixel. `@jax.jit` compiles and accelerates the call.

```python
@jax.jit
def render_image(mu_array, theta_array, scaling_array, color_array, coords):
    """Render the image from the parameters."""
    gaussians = compute_gaussians(mu_array, theta_array, scaling_array, coords)
    # weighted average of gaussians
    rendered_image = jnp.matmul(gaussians, color_array) / (
        jnp.sum(gaussians, axis=-1, keepdims=True) + 1e-6
    )
    return jnp.clip(rendered_image, 0, 255)
```

The image below illustrates the rendering of two random Gaussians. Notice how the colors blend at the intersection. High-quality image rendering typically uses tens of thousands of Gaussians.

![](gs.png)

The function `render_image` is differentiable. We compute the loss by comparing the reconstructed image with the target image. Either $L_1$ loss or MSE works. It is straightforward to implement the training loop in the `main` function. Summary:

- Initialize the Gaussians
- Loop:
  - Render the image
  - Compute L1 loss and gradients
  - Update Gaussian parameters

```python
_SIZE = 128
_ITERATIONS = 1000
_GS_INITIAL_NUM = 5000

def main():
    img = Image.open("input.jpg")
    img_array = np.array(img)
    key = jax.random.PRNGKey(0)
    key_mu, key_theta, key_scaling, key_color = jax.random.split(key, 4)
    mu_array = jax.random.uniform(key_mu, (_GS_NUM, 2), minval=0.0, maxval=1.0)
    theta_array = jax.random.uniform(key_theta, (_GS_NUM, 1), minval=0.0, maxval=jnp.pi)
    scaling_array = jax.random.uniform(
        key_scaling, (_GS_NUM, 2), minval=0.0, maxval=0.1
    )
    color_array = jax.random.uniform(key_color, (_GS_NUM, 3), minval=0.0, maxval=255.0)

    # create a grid of x, y coordinates
    x = jnp.linspace(0, 1.0, _SIZE)
    y = jnp.linspace(0, 1.0, _SIZE)
    xx, yy = jnp.meshgrid(x, y)
    coords = jnp.stack([xx, yy], axis=-1)  # shape (_SIZE, _SIZE, 2)

    learning_rate = 0.001
    optimizer = optax.adam(learning_rate)
    params = (mu_array, theta_array, scaling_array, color_array)
    opt_state = optimizer.init(params)

    def loss_fn(params):
        rendered = render_image(
            *params,
            coords,
        )
        return jnp.mean(jnp.abs(rendered - img_array))

    def update(params, opt_state):
        grads = jax.grad(loss_fn)(params)
        updates, opt_state = optimizer.update(grads, opt_state)
        new_params = optax.apply_updates(params, updates)
        return new_params, opt_state

    for i in range(_ITERATIONS):
        params, opt_state = update(params, opt_state)
        current_loss = loss_fn(params)
```

You can find the complete code here:  
[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/tomondev/image-gs-toy-example)

## Target Image

Here is the target image (128Ã—128):

![](input.jpg)

## Results

The first image is the target image. The last image is from iteration 1000.

![](progression.png)

### Animation

![](progression.gif)

## Important Notes

The current implementation is inefficient and overly simplistic. Improvements can be made in the following areas:

To enhance efficiency, we can avoid evaluating all Gaussians for every pixel. Gaussians far from a pixel contribute negligibly to its color. A more efficient approach would be to select the K Gaussians with the highest density at a pixel. However, identifying and managing K Gaussians per pixel may be too resource-intensive. A pragmatic solution is to generate K Gaussians per tile, while ensuring Gaussians spanning multiple tiles are handled properly.

Random initialization is not optimal. We can initialize Gaussian geometry and color based on pixel intensity and gradients in the target image. This likely leads to faster convergence and better reconstruction quality.

The number of Gaussians does not need to be fixed. In each iteration, we can remove Gaussians whose contributions are negligible and create new ones at pixels with high reconstruction error.

In summary, this is a basic example illustrating how Gaussian splatting works. Many improvements can be implemented to render a high-quality image more efficiently.
