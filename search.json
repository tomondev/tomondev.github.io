[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "On Notes",
    "section": "",
    "text": "Date\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nJan 1, 2020\n\n\nPost 1\n\n\n \n\n\n\n\n\n\nJan 1, 2020\n\n\nOn Notes\n\n\ntest author\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/colab/index.html",
    "href": "posts/colab/index.html",
    "title": "On Notes",
    "section": "",
    "text": "Show the code\n\n\n!pip install clu\n\n\n\nCollecting clu\n\n  Downloading clu-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from clu) (1.4.0)\n\nRequirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from clu) (1.12.2)\n\nRequirement already satisfied: flax in /usr/local/lib/python3.11/dist-packages (from clu) (0.10.6)\n\nRequirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from clu) (0.5.2)\n\nRequirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from clu) (0.5.1)\n\nCollecting ml-collections (from clu)\n\n  Downloading ml_collections-1.1.0-py3-none-any.whl.metadata (22 kB)\n\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from clu) (2.0.2)\n\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clu) (24.2)\n\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from clu) (4.14.1)\n\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from clu) (1.17.2)\n\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]-&gt;clu) (2025.3.2)\n\nRequirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]-&gt;clu) (6.5.2)\n\nRequirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]-&gt;clu) (3.23.0)\n\nRequirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from flax-&gt;clu) (1.1.1)\n\nRequirement already satisfied: optax in /usr/local/lib/python3.11/dist-packages (from flax-&gt;clu) (0.2.5)\n\nRequirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.11/dist-packages (from flax-&gt;clu) (0.11.16)\n\nRequirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from flax-&gt;clu) (0.1.74)\n\nRequirement already satisfied: rich&gt;=11.1 in /usr/local/lib/python3.11/dist-packages (from flax-&gt;clu) (13.9.4)\n\nRequirement already satisfied: PyYAML&gt;=5.4.1 in /usr/local/lib/python3.11/dist-packages (from flax-&gt;clu) (6.0.2)\n\nRequirement already satisfied: treescope&gt;=0.1.7 in /usr/local/lib/python3.11/dist-packages (from flax-&gt;clu) (0.1.9)\n\nRequirement already satisfied: ml_dtypes&gt;=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax-&gt;clu) (0.4.1)\n\nRequirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax-&gt;clu) (3.4.0)\n\nRequirement already satisfied: scipy&gt;=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax-&gt;clu) (1.15.3)\n\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich&gt;=11.1-&gt;flax-&gt;clu) (3.0.0)\n\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich&gt;=11.1-&gt;flax-&gt;clu) (2.19.2)\n\nRequirement already satisfied: chex&gt;=0.1.87 in /usr/local/lib/python3.11/dist-packages (from optax-&gt;flax-&gt;clu) (0.1.89)\n\nRequirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint-&gt;flax-&gt;clu) (1.6.0)\n\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint-&gt;flax-&gt;clu) (5.29.5)\n\nRequirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint-&gt;flax-&gt;clu) (4.12.3)\n\nRequirement already satisfied: simplejson&gt;=3.16.0 in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint-&gt;flax-&gt;clu) (3.20.1)\n\nRequirement already satisfied: toolz&gt;=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex&gt;=0.1.87-&gt;optax-&gt;flax-&gt;clu) (0.12.1)\n\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich&gt;=11.1-&gt;flax-&gt;clu) (0.1.2)\n\nDownloading clu-0.0.12-py3-none-any.whl (101 kB)\n\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.8/101.8 kB 4.7 MB/s eta 0:00:00\n\nDownloading ml_collections-1.1.0-py3-none-any.whl (76 kB)\n\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.7/76.7 kB 7.8 MB/s eta 0:00:00\n\nInstalling collected packages: ml-collections, clu\n\nSuccessfully installed clu-0.0.12 ml-collections-1.1.0\n\n\n\n\n\nimport jax.numpy as jnp\nimport jax\nimport flax.linen as nn\nimport flax\nimport tensorflow_datasets as tfds\nfrom functools import partial\nimport numpy as np\nfrom flax.training import train_state  # Useful dataclass to keep train state\nfrom flax import struct  # Flax dataclasses\nimport optax\nfrom clu import metrics\nfrom typing import Sequence, Any\n\n\n\nShow the code\nimport tensorflow_datasets as tfds  # TFDS for MNIST\nimport tensorflow as tf  # TensorFlow operations\n\ntf.random.set_seed(0)  # set random seed for reproducibility\n\nnum_epochs = 10\nbatch_size = 32\n\ntrain_ds: tf.data.Dataset = tfds.load('mnist', split='train')\ntest_ds: tf.data.Dataset = tfds.load('mnist', split='test')\n\ntrain_ds = train_ds.map(\n  lambda sample: {\n    'image': tf.cast(sample['image'], tf.float32) / 255,\n    'label': sample['label'],\n  }\n)  # normalize train set\ntest_ds = test_ds.map(\n  lambda sample: {\n    'image': tf.cast(sample['image'], tf.float32) / 255,\n    'label': sample['label'],\n  }\n)  # normalize test set\n\n# create shuffled dataset by allocating a buffer size of 1024 to randomly draw elements from\ntrain_ds = train_ds.repeat(num_epochs).shuffle(1024)\n# group into batches of batch_size and skip incomplete batch, prefetch the next sample to improve latency\ntrain_ds = train_ds.batch(batch_size, drop_remainder=True).prefetch(1)\n# create shuffled dataset by allocating a buffer size of 1024 to randomly draw elements from\ntest_ds = test_ds.shuffle(1024)\n# group into batches of batch_size and skip incomplete batch, prefetch the next sample to improve latency\ntest_ds = test_ds.batch(batch_size, drop_remainder=True).prefetch(1)\n\n\nWARNING:absl:Variant folder /root/tensorflow_datasets/mnist/3.0.1 has no dataset_info.json\n\n\nDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/mnist/3.0.1...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\n\n\n\n\nShow the code\nclass down_block(nn.Module):\n\n    @nn.compact\n    def __call__(self,\n                 inputs,\n                 n_filter=32,\n                 max_pooling=True,\n                 training=True):\n        conv = nn.Conv(\n            n_filter,\n            3,  # filter size\n            padding='SAME',\n            kernel_init=nn.initializers.he_normal())(inputs)\n        skip_connection = conv\n        conv = nn.BatchNorm(use_running_average=not training)(conv)\n        conv = nn.relu(conv)\n        conv = nn.Conv(\n            n_filter,\n            3,  # filter size\n            padding='SAME',\n            kernel_init=nn.initializers.he_normal())(conv)\n        conv = nn.relu(conv)\n\n        if max_pooling:\n            next_layer = nn.max_pool(conv, window_shape=(2, 2), padding='SAME')\n        else:\n            next_layer = conv\n        return next_layer, skip_connection\n\n\nclass up_block(nn.Module):\n\n    @nn.compact\n    def __call__(self, inputs, skip_connection, filters, training=True):\n        if skip_connection is None:\n            x = inputs\n        else:\n            x = jnp.concatenate([inputs, skip_connection], axis=3)\n        x = nn.Conv(filters, 3, padding='SAME')(x)\n        x = nn.BatchNorm(use_running_average=not training)(x)\n        x = nn.relu(x)\n        x = nn.Conv(filters, 3, padding='SAME')(x)\n        x = nn.BatchNorm(use_running_average=not training)(x)\n        x = nn.relu(x)\n        return x\n\n\nclass UNet(nn.Module):\n\n    @nn.compact\n    def __call__(self, input):\n        filter = [64, 128, 256]\n        # encode\n        x, temp1 = down_block()(input, filter[0])\n        x, temp2 = down_block()(x, filter[1])\n        x, _ = down_block()(x, filter[2], max_pooling=False)\n        # decode\n        x = up_block()(x, temp2, filter[1])\n        x = up_block()(x, temp1, filter[0])\n        x = up_block()(x, None, 1)\n        return x\n\n\n\nunet = UNet()\n# print(\n#     m.tabulate(jax.random.key(0),\n#                jnp.ones((1, 28, 28, 1)),\n#                compute_flops=True,\n#                compute_vjp_flops=True))\n\n\n\n\nShow the code\n@struct.dataclass\nclass Metrics(metrics.Collection):\n    loss: metrics.Average.from_output('loss')\n\n\nclass TrainState(train_state.TrainState):\n    metrics: Metrics\n    batch_stats: Any\n\n\ndef create_train_state(module, rng, learning_rate, momentum):\n    \"\"\"Creates an initial `TrainState`.\"\"\"\n    variables = module.init(rng, jnp.ones([1, 28, 28, 1]))\n    params = variables[\n        'params']  # initialize parameters by passing a template image\n    batch_stats = variables[\n        'batch_stats']  # initialize batch_stats by passing a template image\n    tx = optax.sgd(learning_rate, momentum)\n    return TrainState.create(apply_fn=module.apply,\n                             params=params,\n                             batch_stats=batch_stats,\n                             tx=tx,\n                             metrics=Metrics.empty())\n\n\n@jax.jit\ndef train_step(state, batch):\n    \"\"\"Train for a single step.\"\"\"\n\n    def loss_fn(params):\n        predicted, updates = state.apply_fn(\n            {\n                'params': params,\n                'batch_stats': state.batch_stats\n            },\n            batch['image'],\n            mutable=['batch_stats'],\n            rngs={'dropout': jax.random.key(1)})\n        loss = optax.losses.l2_loss(predictions=predicted,\n                                    targets=batch['image']).mean()\n        return loss, (predicted, updates)\n\n    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n    (loss, (predicted, updates)), grads = grad_fn(state.params)\n    state = state.apply_gradients(grads=grads)\n    state = state.replace(batch_stats=updates['batch_stats'])\n    return state\n\n\n@jax.jit\ndef compute_metrics(*, state, batch):\n    predicted, updates = state.apply_fn(\n        {\n            'params': state.params,\n            'batch_stats': state.batch_stats\n        },\n        batch['image'],\n        mutable=['batch_stats'],\n        rngs={'dropout': jax.random.key(1)})\n    loss = optax.losses.l2_loss(predictions=predicted,\n                                targets=batch['image']).mean()\n    metric_updates = state.metrics.single_from_model_output(\n        predictions=predicted, targets=batch['label'], loss=loss)\n    metrics = state.metrics.merge(metric_updates)\n    state = state.replace(metrics=metrics)\n    return state\n\n\nnum_epochs = 10\nbatch_size = 32\n\n# train_ds, test_ds = get_datasets(num_epochs, batch_size)\n\n# tf.random.set_seed(0)\ninit_rng = jax.random.key(0)\nlearning_rate = 0.01\nmomentum = 0.9\nstate = create_train_state(unet, init_rng, learning_rate, momentum)\ndel init_rng  # Must not be used anymore.\n\n# since train_ds is replicated num_epochs times in get_datasets(), we divide by num_epochs\nnum_steps_per_epoch = train_ds.cardinality().numpy() // num_epochs\nnum_steps_per_epoch = 2\nprint(num_steps_per_epoch)\n\nmetrics_history = {\n    'train_loss': [],\n    'test_loss': [],\n}\n\n\ntest_summary_writer = tf.summary.create_file_writer('test/logdir')\n\nfor step, batch in enumerate(train_ds.as_numpy_iterator()):\n    if step &gt; 20:\n      break\n\n    # Run optimization steps over training batches and compute batch metrics\n    state = train_step(\n        state, batch\n    )  # get updated train state (which contains the updated parameters)\n    state = compute_metrics(state=state,\n                            batch=batch)  # aggregate batch metrics\n\n    if (step + 1) % num_steps_per_epoch == 0:  # one training epoch has passed\n        for metric, value in state.metrics.compute().items(\n        ):  # compute metrics\n            metrics_history[f'train_{metric}'].append(value)  # record metrics\n            with test_summary_writer.as_default():\n              tf.summary.scalar(\n                'train/loss', value, step=step\n              )\n\n        state = state.replace(metrics=state.metrics.empty()\n                              )  # reset train_metrics for next training epoch\n\n        # Compute metrics on the test set after each training epoch\n        test_state = state\n        for test_batch in test_ds.as_numpy_iterator():\n            test_state = compute_metrics(state=test_state, batch=test_batch)\n\n        for metric, value in test_state.metrics.compute().items():\n            metrics_history[f'test_{metric}'].append(value)\n            with test_summary_writer.as_default():\n              tf.summary.scalar(\n                'test/loss', value, step=step\n              )\n\n        print(f\"train epoch: {(step+1) // num_steps_per_epoch}, \"\n              f\"loss: {metrics_history['train_loss'][-1]}, \")\n        print(f\"test epoch: {(step+1) // num_steps_per_epoch}, \"\n              f\"loss: {metrics_history['test_loss'][-1]}, \")\n        # writer.add_image('images', grid, 0)\n        # writer.add_graph(model, images)\n\n\n2\ntrain epoch: 1, loss: 0.23241570591926575, \ntest epoch: 1, loss: 0.19601216912269592, \ntrain epoch: 2, loss: 0.1533064991235733, \ntest epoch: 2, loss: 0.15013296902179718, \ntrain epoch: 3, loss: 0.12592682242393494, \ntest epoch: 3, loss: 0.11877958476543427, \ntrain epoch: 4, loss: 0.0943879559636116, \ntest epoch: 4, loss: 0.08934623003005981, \ntrain epoch: 5, loss: 0.06853969395160675, \ntest epoch: 5, loss: 0.06333110481500626, \ntrain epoch: 6, loss: 0.04726167768239975, \ntest epoch: 6, loss: 0.04230469465255737, \ntrain epoch: 7, loss: 0.02610059641301632, \ntest epoch: 7, loss: 0.026588963344693184, \ntrain epoch: 8, loss: 0.01785922423005104, \ntest epoch: 8, loss: 0.015624734573066235, \ntrain epoch: 9, loss: 0.01218641921877861, \ntest epoch: 9, loss: 0.008579996414482594, \ntrain epoch: 10, loss: 0.006207008380442858, \ntest epoch: 10, loss: 0.004718703217804432, \n\n\n\n\nShow the code\n# %load_ext tensorboard\n%tensorboard --logdir 'test/logdir'\n\n\n\n\n\n\n\nShow the code\nnp.random.rand((1,28,28,))\n\n\n\n\nShow the code\nconv = nn.ConvTranspose(features=2, kernel_size=3)\nconv.init_with_output(jax.random.key(0), x)"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post 1",
    "section": "",
    "text": "TESTSSDF TESTSSDFTESTSSDFTESTSSDFTESTSSDFTESTSSDF"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Tom. I write small examples on various interesting topics.\nIf you find mistakes in my posts (which is very likely), please file issues."
  }
]