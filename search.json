[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "On Notes",
    "section": "",
    "text": "Date\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nAug 1, 2025\n\n\nA Toy Example of 2D Gaussian Splatting in JAX\n\n\nTom \n\n\n\n\n\n\nJan 1, 2025\n\n\nDiffusion Reverse Samplers Approximate Gaussian Distributions\n\n\nTom \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/diffusion_reverse_sampler_gaussian/index.html",
    "href": "posts/diffusion_reverse_sampler_gaussian/index.html",
    "title": "Diffusion Reverse Samplers Approximate Gaussian Distributions",
    "section": "",
    "text": "In diffusion models, let’s assume that \\(x_0\\) is the data sample (for example, an image) and \\(x_T\\) is the noise. The forward process is defined as \\[\nx_t = x_{t-1} + z_{t-1}\n\\] where \\(z_{t-1}\\) is independent Gaussian noise \\(\\mathcal{N}(0, \\sigma^2)\\).\nThe construction of a reverse sampler relies on a key property: for small \\(\\sigma\\) values, the conditional distribution \\(p(x_{t-1}|x_t)\\) closely approximates a Gaussian distribution. In this post, we will write a small experiment to verify this property.\nLet’s assume that \\(p(x_{t-1})\\) is a non-trivial distribution, for example, a mixture of two Gaussian distributions: \\(\\mathcal{N}(x|\\mu_1,\\sigma_1^2)\\) and \\(\\mathcal{N}(x|\\mu_2,\\sigma_2^2)\\) with equal weights. The pdf of \\(x_{t-1}\\) can be written as \\[\np(x) = \\dfrac{1}{2}\\dfrac{1}{(2\\pi\\sigma_1^2)^{1/2}}\\exp \\left\\{-\\dfrac{1}{2\\sigma_1^2}(x-\\mu_1)^2\\right\\} + \\dfrac{1}{2}\\dfrac{1}{(2\\pi\\sigma_2^2)^{1/2}}\\exp \\left\\{-\\dfrac{1}{2\\sigma_2^2}(x-\\mu_2)^2\\right\\}\n\\] Let’s use \\(\\mu_1=1, \\sigma_1=0.6\\) and \\(\\mu_2=2, \\sigma_2=0.2\\). The pdf looks like this:\n\nAccording to eq(1), we also know that the conditional distribution \\(p(x_t|x_{t-1})\\) is just a Gaussian. We are interested in the conditional distribution \\(p(x_{t-1}|x_{t})\\). According to Bayes’ rule, it can be written as: \\[\np(x_{t-1}|x_t) = \\dfrac{p(x_{t}|x_{t-1})p(x_{t-1})}{\\int p(x_{t}|x_{t-1})p(x_{t-1})dx_{t-1}}\n\\] We already know \\(p(x_t|x_{t-1})\\) and \\(p(x_{t-1})\\), so the only thing we need to figure out is the denominator. But notice that the denominator is a constant with respect to \\(x_{t-1}\\). Therefore, we can just look at the numerator to see how close it is to a Gaussian distribution. Below, we plot the unnormalized pdf defined by the numerator at different \\(\\sigma\\) values:\nWe fix \\(x_{t}=1\\) and set \\(\\sigma\\) to 0.5, 0.3, and 0.1. At 0.1, the reverse conditional distribution is almost the same as a Gaussian.\n\nCurious readers can find the mathematical proof in 2406.08929."
  },
  {
    "objectID": "posts/image-2dgs/index.html",
    "href": "posts/image-2dgs/index.html",
    "title": "A Toy Example of 2D Gaussian Splatting in JAX",
    "section": "",
    "text": "3D Gaussian Splatting (3DGS) is a rendering technique for photorealistic view synthesis. In this post, we will create a toy example of 2D Gaussian splatting (2DGS). We will train a 2DGS model to render a small image. This 2D version strips away much of the complexity found in 3DGS, making the core ideas much easier to understand.\nHere are some key differences between 2DGS and 3DGS:"
  },
  {
    "objectID": "posts/image-2dgs/index.html#problem-formulation",
    "href": "posts/image-2dgs/index.html#problem-formulation",
    "title": "A Toy Example of 2D Gaussian Splatting in JAX",
    "section": "Problem Formulation",
    "text": "Problem Formulation\nA 2D Gaussian has a mean vector \\(\\mu\\in\\mathbb{R}^2\\) and a covariance matrix \\(\\Sigma\\in\\mathbb{R}^{2 \\times 2}\\). The density is expressed as \\[\nG(x)=\\exp\\left(-\\dfrac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\right)\n\\] where \\(x\\) is the pixel location. Note that this is not a probability density, so it doesn’t need to be normalized.\nUsually, people use \\(\\Sigma=RSS^TR^T\\) to ensure \\(\\Sigma\\) is always positive semi-definite during training. \\(R\\in\\mathbb{R}^{2\\times2}\\) is a rotation matrix, and \\(S\\in\\mathbb{R}^{2\\times2}\\) is a scaling matrix. \\(R\\) is parameterized by an angle \\(\\theta\\in[0,\\pi]\\), and \\(S\\) is determined by a scaling vector \\(s\\in\\mathbb{R}^2_+\\).\nNext, we need to assign a color to each Gaussian. Unlike 3DGS, which uses spherical harmonics, we only need a vector \\(c\\in\\mathbb{R}^3\\) to represent the RGB values of a 2D Gaussian.\nIn the end, a 2D Gaussian primitive is characterized by \\((\\mu, \\theta, s, c)\\). Each pixel value \\(\\mathbf{RGB}(x)\\) is determined by a weighted average of all the Gaussians where the weights are computed by the density function \\(G(x)\\): \\[\n\\mathbf{RGB}(x)=\\dfrac{1}{\\sum_i G_i(x)}\\sum_i G_i(x)c_i\n\\]"
  },
  {
    "objectID": "posts/image-2dgs/index.html#target-image",
    "href": "posts/image-2dgs/index.html#target-image",
    "title": "A Toy Example of 2D Gaussian Splatting in JAX",
    "section": "Target Image",
    "text": "Target Image\nHere is our target image with a size of 128x128."
  },
  {
    "objectID": "posts/image-2dgs/index.html#implementation",
    "href": "posts/image-2dgs/index.html#implementation",
    "title": "A Toy Example of 2D Gaussian Splatting in JAX",
    "section": "Implementation",
    "text": "Implementation\nFirst, we randomly initialize all the Gaussian parameters. We use a total of 5000 Gaussians. Using more Gaussians improves quality but also increases memory usage.\n_GS_NUM = 5000\nkey = jax.random.PRNGKey(0)\nkey_mu, key_theta, key_scaling, key_color = jax.random.split(key, 4)\nmu_array = jax.random.uniform(key_mu, (_GS_NUM, 2), minval=0.0, maxval=1.0)\ntheta_array = jax.random.uniform(key_theta, (_GS_NUM, 1), minval=0.0, maxval=jnp.pi)\nscaling_array = jax.random.uniform(\n    key_scaling, (_GS_NUM, 2), minval=0.0, maxval=0.1\n)\ncolor_array = jax.random.uniform(key_color, (_GS_NUM, 3), minval=0.0, maxval=255.0)\nNext, we implement the key part of the training code: a function to compute the Gaussian density. We will implement the function for computing the density of a single Gaussian at a single pixel location. We then use three layers of jax.vmap to vectorize the function, allowing a single call to compute the density for all Gaussians at all locations.\n@partial(jax.vmap, in_axes=(None, None, None, 0))  # map over height dimension\n@partial(jax.vmap, in_axes=(None, None, None, 0))  # map over width dimension\n@partial(jax.vmap, in_axes=(0, 0, 0, None))  # map over gaussians\ndef compute_gaussians(mu, theta, scaling, coord):\n    \"\"\"Compute the value of a single Gaussian at the given coordinate.\"\"\"\n    # rotation matrix\n    # clip theta to the range [0, pi]\n    theta = jnp.clip(theta, 0.0, jnp.pi)\n    c = jnp.cos(theta[0])\n    s = jnp.sin(theta[0])\n    R = jnp.array([[c, -s], [s, c]])\n    # scaling matrix\n    # clip scaling to be positive\n    scaling = jnp.clip(scaling, min=1e-6, max=None)\n    S = jnp.diag(scaling)\n    # covariance matrix\n    # add small value for numerical stability\n    Sigma = R @ S @ S @ R.T + 1e-6 * jnp.eye(2)\n    diff = coord - mu\n    exponent = -0.5 * diff @ jnp.linalg.inv(Sigma) @ diff.T\n    return jnp.exp(exponent)\nWith compute_gaussians implemented, it is easy to finish the rendering function. It basically computes a weighted average of all Gaussian colors for each pixel. @jax.jit is needed to compile and accelerate the rendering call.\n@jax.jit\ndef render_image(mu_array, theta_array, scaling_array, color_array, coords):\n    \"\"\"Render the image from the parameters.\"\"\"\n    gaussians = compute_gaussians(mu_array, theta_array, scaling_array, coords)\n    # weighted average of gaussians\n    rendered_image = jnp.matmul(gaussians, color_array) / (\n        jnp.sum(gaussians, axis=-1, keepdims=True) + 1e-6\n    )\n    return jnp.clip(rendered_image, 0, 255)\nrender_image is implemented in a differentiable way. We can compute the loss by comparing the reconstructed image against the target image. Either \\(L_1\\) loss or MSE will work. It is now straightforward to implement the training loop in the main function. We can summarize the training code as follows:\n\nInitialize the gaussians\nloop\n\nrender the image\ncompute l1 loss and gradients\nupdate gaussian parameters\n\n\n_SIZE = 128\n_ITERATIONS = 1000\n_GS_INITIAL_NUM = 5000\n\ndef main():\n    img = Image.open(\"input.jpg\")\n    img_array = np.array(img)\n    key = jax.random.PRNGKey(0)\n    key_mu, key_theta, key_scaling, key_color = jax.random.split(key, 4)\n    mu_array = jax.random.uniform(key_mu, (_GS_NUM, 2), minval=0.0, maxval=1.0)\n    theta_array = jax.random.uniform(key_theta, (_GS_NUM, 1), minval=0.0, maxval=jnp.pi)\n    scaling_array = jax.random.uniform(\n        key_scaling, (_GS_NUM, 2), minval=0.0, maxval=0.1\n    )\n    color_array = jax.random.uniform(key_color, (_GS_NUM, 3), minval=0.0, maxval=255.0)\n\n    # create a grid of x, y coordinates\n    x = jnp.linspace(0, 1.0, _SIZE)\n    y = jnp.linspace(0, 1.0, _SIZE)\n    xx, yy = jnp.meshgrid(x, y)\n    coords = jnp.stack([xx, yy], axis=-1)  # shape (_SIZE, _SIZE, 2)\n\n    learning_rate = 0.001\n    optimizer = optax.adam(learning_rate)\n    params = (mu_array, theta_array, scaling_array, color_array)\n    opt_state = optimizer.init(params)\n\n    def loss_fn(params):\n        rendered = render_image(\n            *params,\n            coords,\n        )\n        return jnp.mean(jnp.abs(rendered - img_array))\n\n    def update(params, opt_state):\n        grads = jax.grad(loss_fn)(params)\n        updates, opt_state = optimizer.update(grads, opt_state)\n        new_params = optax.apply_updates(params, updates)\n        return new_params, opt_state\n\n    for i in range(_ITERATIONS):\n        params, opt_state = update(params, opt_state)\n        current_loss = loss_fn(params)\nYou can find the complete code in"
  },
  {
    "objectID": "posts/image-2dgs/index.html#results",
    "href": "posts/image-2dgs/index.html#results",
    "title": "A Toy Example of 2D Gaussian Splatting in JAX",
    "section": "Results",
    "text": "Results\nThe first image is the target image. The last image is from iteration 1000.\n\n\nanimation"
  },
  {
    "objectID": "posts/image-2dgs/index.html#important-notes",
    "href": "posts/image-2dgs/index.html#important-notes",
    "title": "A Toy Example of 2D Gaussian Splatting in JAX",
    "section": "Important Notes",
    "text": "Important Notes\nThe current implementation is inefficient and overly simplistic. Improvements can be made in the following aspects:\nTo enhance efficiency, we can avoid evaluating all Gaussians for every pixel. Gaussians located far from a pixel contribute negligibly to its color. A more efficient approach involves selecting the K Gaussians with the highest density at a given pixel. However, identifying and managing K Gaussians for each individual pixel might be too resource-intensive. A more pragmatic solution is to generate K Gaussians for a specific tile, while ensuring that Gaussians spanning multiple tiles are managed appropriately.\nIt is not optimal to initialize the Gaussians randomly. Instead, we can initialize the geometry and color of the Gaussians according to pixel intensity and gradients in the target image. This will likely lead to faster convergence and better reconstruction quality.\nLast but not least, the number of Gaussians doesn’t have to be fixed. In each iteration, we can remove some Gaussians whose contributions are negligible and create new Gaussians at pixels where reconstruction errors are high.\nTo summarize, this is a basic example that illustrates how Gaussian splatting works. Many improvements can be added to render a high-quality image more efficiently."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Tom. I write small examples on various interesting topics.\nIf you find mistakes in my posts, please file issues."
  }
]